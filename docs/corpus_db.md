# å¤§è¦æ¨¡ã‚³ãƒ¼ãƒ‘ã‚¹æ§‹ç¯‰ã‚¬ã‚¤ãƒ‰ (CorpusDB)

æœ¬ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã¯ã€`CorpusDB` ã‚¯ãƒ©ã‚¹ã‚’ä½¿ç”¨ã—ã¦ã€å¤§é‡ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ Google Colaboratory ä¸Šã§å®‰å…¨ã«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åŒ–ã™ã‚‹æ–¹æ³•ã‚’èª¬æ˜Žã—ã¾ã™ã€‚

## æ¦‚è¦ã¨ãƒ¡ãƒªãƒƒãƒˆ

æ•°ä¸‡ã€œæ•°ç™¾ä¸‡ãƒ•ã‚¡ã‚¤ãƒ«è¦æ¨¡ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æ‰±ã†å ´åˆã€ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€åº¦ã«ãƒ¡ãƒ¢ãƒªï¼ˆPandas DataFrameï¼‰ã«èª­ã¿è¾¼ã‚€ã¨ã€Colab ã®ãƒ¡ãƒ¢ãƒªåˆ¶é™ã‚’è¶…ãˆã¦ã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã™ã‚‹æã‚ŒãŒã‚ã‚Šã¾ã™ã€‚

`CorpusDB` ã¯ä»¥ä¸‹ã®è¨­è¨ˆã«ã‚ˆã‚Šã€ã“ã®å•é¡Œã‚’è§£æ±ºã—ã¾ã™ã€‚

* **çœãƒ¡ãƒ¢ãƒªè¨­è¨ˆ**: ãƒ†ã‚­ã‚¹ãƒˆã®ä¸­èº«ã¯å‡¦ç†ç›´å‰ã¾ã§èª­ã¿è¾¼ã¾ãšã€ãƒ‘ã‚¹æƒ…å ±ã®ã¿ã‚’å…ˆã«ç®¡ç†ã—ã¾ã™ã€‚
* **å®‰å…¨ãªé€æ¬¡å‡¦ç†**: 1ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã™ã‚‹ã”ã¨ã« DB æŽ¥ç¶šã‚’ã‚³ãƒŸãƒƒãƒˆãƒ»åˆ‡æ–­ã™ã‚‹ãŸã‚ã€ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ã‚„ãƒ‡ãƒ¼ã‚¿æå¤±ã‚’é˜²ãŽã¾ã™ã€‚
* **ä¸­æ–­ãƒ»å†é–‹**: å‡¦ç†çŠ¶æ³ã‚’è¨˜éŒ²ã—ã¦ã„ã‚‹ãŸã‚ã€ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚„ã‚¨ãƒ©ãƒ¼ã§æ­¢ã¾ã£ã¦ã‚‚ã€Œæœªå‡¦ç†ã®ãƒ•ã‚¡ã‚¤ãƒ«ã€ã‹ã‚‰å³åº§ã«å†é–‹ã§ãã¾ã™ã€‚

---

## å®Ÿè¡Œæ‰‹é †

### 1. ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®æº–å‚™

```python
# colab-nlp ã‚’ clone æ¸ˆã¿ã§ãƒ‘ã‚¹ãŒé€šã£ã¦ã„ã‚‹å‰æ
from colab_nlp import tokenize_df, CorpusDB
```

### 2. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®åˆæœŸåŒ–

ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆSQLiteï¼‰ã‚’æŒ‡å®šã—ã¦åˆæœŸåŒ–ã—ã¾ã™ã€‚ Google Drive ä¸Šã®ãƒ‘ã‚¹ã‚’æŒ‡å®šã™ã‚‹ã¨ã€ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒåˆ‡ã‚Œã¦ã‚‚ãƒ‡ãƒ¼ã‚¿ãŒæ®‹ã‚‹ãŸã‚æŽ¨å¥¨ã•ã‚Œã¾ã™ã€‚

```python
# Google Drive ã‚’ãƒžã‚¦ãƒ³ãƒˆï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
from google.colab import drive
drive.mount("/content/drive")

# DBãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’æŒ‡å®šï¼ˆGoogle Drive ä¸Šã®ãƒ‘ã‚¹ã‚’æŽ¨å¥¨ï¼‰
db_path = "/content/drive/MyDrive/nlp_data/my_corpus.db"
db = CorpusDB(db_path)
```

### 3. ãƒ•ã‚¡ã‚¤ãƒ«ã®ç™»éŒ²

æŒ‡å®šã—ãŸãƒ•ã‚©ãƒ«ãƒ€é…ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†å¸°çš„ã«æ¤œç´¢ã—ã€ãƒ‘ã‚¹æƒ…å ±ã®ã¿ã‚’ DB ã«ç™»éŒ²ã—ã¾ã™ã€‚ ã“ã®æ®µéšŽã§ã¯ãƒ†ã‚­ã‚¹ãƒˆã®ä¸­èº«ã‚’èª­ã¿è¾¼ã¾ãªã„ãŸã‚ã€å¤§é‡ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã£ã¦ã‚‚é«˜é€Ÿã«å®Œäº†ã—ã¾ã™ã€‚

```python
# ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒæ ¼ç´ã•ã‚Œã¦ã„ã‚‹ãƒ«ãƒ¼ãƒˆãƒ•ã‚©ãƒ«ãƒ€
root_dir = "/content/drive/MyDrive/path/to/text_data"

# æ‹¡å¼µå­ã‚’æŒ‡å®šã—ã¦ç™»éŒ²ï¼ˆãƒªã‚¹ãƒˆã§è¤‡æ•°æŒ‡å®šå¯èƒ½ï¼‰
db.register_files(root_dir, exts=["*.txt"])
```

### 4. ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶é–¢æ•°ã®å®šç¾©

CorpusDB ã¯å‡¦ç†ã®å†…éƒ¨ã§ã€ŒDataFrame ã‚’å—ã‘å–ã‚Šã€å½¢æ…‹ç´ è§£æžæ¸ˆã¿ã® DataFrame ã‚’è¿”ã™é–¢æ•°ã€ã‚’å¿…è¦ã¨ã—ã¾ã™ã€‚ `tokenize_df` ã®è¨­å®šï¼ˆã‚¨ãƒ³ã‚¸ãƒ³ã‚„è¾žæ›¸ã€ãƒ•ã‚£ãƒ«ã‚¿è¨­å®šãªã©ï¼‰ã‚’å›ºå®šã—ãŸãƒ©ãƒƒãƒ‘ãƒ¼é–¢æ•°ã‚’å®šç¾©ã—ã¾ã™ã€‚`tokenize_df` ã®è©³ç´° (engin ã«ã‚ã‚ã›ã¦ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ãªã‘ã‚Œã°ãªã‚‰ãªã„ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãªã©) ã¯ [`tokenization.md`](./tokenization.md) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

```python
# engine ã«ã‚ã‚ã›ãŸè¿½åŠ ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆãŒå¿…è¦ã€‚ã“ã“ã§ã¯ã€å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã¨ã„ã†è¦ä»¶ã«ã‚ã‚ã›ã¦ sudachi ã‚’æŽ¡ç”¨ã™ã‚‹
!pip install sudachipy sudachidict_core

# ãƒ©ãƒƒãƒ‘ãƒ¼é–¢æ•°ã®å®šç¾© (id_col, text_col, token_id_col ã¯çœç•¥ã—ã¦ã‚‚ã‚ˆã„)
def my_tokenizer(df):
    """
    CorpusDB ç”¨ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºé–¢æ•°
    å…¥åŠ›: df (columns: [doc_id, text])
    å‡ºåŠ›: df (columns: [doc_id, word, pos, token_info, ...])
    """
    return tokenize_df(
        df,
        engine="sudachi",     # ã¾ãŸã¯ "janome"
        id_col="doc_id",      # DBã®ã‚«ãƒ©ãƒ åã¨åˆã‚ã›ã‚‹
        text_col="text",
        token_id_col="token_id",
    )
```

### 5. å‡¦ç†ã®å®Ÿè¡Œ (æ§‹ç¯‰é–‹å§‹)

æœªå‡¦ç†ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é †æ¬¡å‡¦ç†ã—ã¾ã™ã€‚ ã“ã®ã‚»ãƒ«ã¯ä½•åº¦å®Ÿè¡Œã—ã¦ã‚‚å¤§ä¸ˆå¤«ã§ã™ã€‚é€”ä¸­ã§ Colab ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ãŸã‚Šã‚¨ãƒ©ãƒ¼ã§æ­¢ã¾ã£ãŸã‚Šã—ã¦ã‚‚ã€å†åº¦å®Ÿè¡Œã™ã‚Œã°ã€Œã¾ã çµ‚ã‚ã£ã¦ã„ãªã„ãƒ•ã‚¡ã‚¤ãƒ«ã€ã‹ã‚‰å‡¦ç†ã‚’å†é–‹ã—ã¾ã™ã€‚

```python
# æ§‹ç¯‰å‡¦ç†ã®å®Ÿè¡Œ
db.process_queue(my_tokenizer)
```

---

## æ§‹ç¯‰ã—ãŸãƒ‡ãƒ¼ã‚¿ã®åˆ©ç”¨æ–¹æ³•

ä½œæˆã•ã‚ŒãŸ corpus.db ã¯ SQLite å½¢å¼ã§ã™ã€‚pandas ã¨ sqlite3 ã‚’ä½¿ã£ã¦ç°¡å˜ã«ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã§ãã¾ã™ã€‚

### ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿å‡ºã—ä¾‹

`token_info` ã‚«ãƒ©ãƒ ã¯ JSON æ–‡å­—åˆ—ã¨ã—ã¦ä¿å­˜ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€èª­ã¿å‡ºã—æ™‚ã« `json.loads` ã§è¾žæ›¸åž‹ã«å¾©å…ƒã™ã‚‹ã¨ä¾¿åˆ©ã§ã™ã€‚

```python
import sqlite3
import pandas as pd
import json

db_path = "/content/drive/MyDrive/nlp_data/my_corpus.db"

with sqlite3.connect(db_path) as con:
    # ä¾‹ï¼šç‰¹å®šã®å˜èªžã‚’å«ã‚€ãƒˆãƒ¼ã‚¯ãƒ³æƒ…å ±ã‚’å–å¾—
    df = pd.read_sql("""
        SELECT 
            d.rel_path, 
            t.word, 
            t.pos, 
            t.token_info
        FROM tokens t
        JOIN documents d ON t.doc_id = d.doc_id
        WHERE t.word = 'çŒ«'
        LIMIT 10
    """, con)

# JSON æ–‡å­—åˆ—ã‚’è¾žæ›¸åž‹ã«å¾©å…ƒ
if not df.empty:
    df["token_info"] = df["token_info"].apply(json.loads)

# çµæžœã®ç¢ºèª
print(df.head())
# df.iloc[0]["token_info"]["reading"] ãªã©ã§ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½
```

## ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ã¨æ¤œç´¢ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

`CorpusDB` ã¯å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚’æ‰±ãˆã‚‹ã‚ˆã†ã«è¨­è¨ˆã•ã‚Œã¦ã„ã¾ã™ãŒã€ãƒ‡ãƒ¼ã‚¿ã®å–ã‚Šå‡ºã—æ–¹ï¼ˆSQLï¼‰ã«ã‚ˆã£ã¦ã¯å‡¦ç†æ™‚é–“ãŒé•·ããªã£ãŸã‚Šã€ãƒ¡ãƒ¢ãƒªä¸è¶³ã§ã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã—ãŸã‚Šã™ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚

ä»¥ä¸‹ã®ãƒªã‚¹ãƒˆã‚’å‚è€ƒã«ã€åŠ¹çŽ‡çš„ãªã‚¯ã‚¨ãƒªã‚’è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚

### âœ… æŽ¨å¥¨ã•ã‚Œã‚‹æ¤œç´¢ï¼ˆé«˜é€Ÿãƒ»å®‰å…¨ï¼‰

ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆç´¢å¼•ï¼‰ãŒåŠ¹ãã‚«ãƒ©ãƒ ã‚’ä½¿ã£ãŸæ¤œç´¢ã¯ã€ãƒ‡ãƒ¼ã‚¿é‡ãŒæ•°å„„è¡Œã«ãªã£ã¦ã‚‚ä¸€çž¬ã§çµ‚ã‚ã‚Šã¾ã™ã€‚

| æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³ | SQLä¾‹ | ç†ç”± |
| :--- | :--- | :--- |
| **å˜èªžã§æ¤œç´¢** | `SELECT * FROM tokens WHERE word = 'çŒ«'` | `word` ã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒã‚ã‚‹ãŸã‚çˆ†é€Ÿã§ã™ã€‚ |
| **å“è©žã§æ¤œç´¢** | `SELECT * FROM tokens WHERE pos = 'åè©ž'` | `pos` ã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒã‚ã‚‹ãŸã‚é«˜é€Ÿã§ã™ã€‚ |
| **æ–‡æ›¸IDã§æ¤œç´¢** | `SELECT * FROM tokens WHERE doc_id = 100` | `doc_id` ã¯ä¸»ã‚­ãƒ¼ã®ä¸€éƒ¨ã§ã‚ã‚Šæœ€é€Ÿã§ã™ã€‚ |
| **è¤‡åˆæ¡ä»¶** | `SELECT * FROM tokens WHERE word = 'çŒ«' AND pos = 'åè©ž'` | ä¸¡æ–¹ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ´»ç”¨ã—ã¦åŠ¹çŽ‡ã‚ˆãçµžã‚Šè¾¼ã¿ã¾ã™ã€‚ |
| **ç¯„å›²æŒ‡å®š** | `SELECT * FROM tokens WHERE doc_id BETWEEN 1 AND 1000` | æ–‡æ›¸ã”ã¨ã®ä¸€æ‹¬å–å¾—ã¯éžå¸¸ã«é«˜é€Ÿã§ã™ã€‚Pythonå´ã§ã®ãƒ«ãƒ¼ãƒ—å‡¦ç†ã«é©ã—ã¦ã„ã¾ã™ã€‚ |

### âš ï¸ é¿ã‘ã‚‹ã¹ãæ¤œç´¢ï¼ˆä½Žé€Ÿãƒ»ãƒ¡ãƒ¢ãƒªæž¯æ¸‡ã®å±é™ºã‚ã‚Šï¼‰

ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒãªã„ã‚«ãƒ©ãƒ ã§ã®æ¤œç´¢ã‚„ã€å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€åº¦ã«èª­ã¿è¾¼ã‚€å‡¦ç†ã¯é¿ã‘ã¦ãã ã•ã„ã€‚

| æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³ | SQLä¾‹ | ç†ç”±ãƒ»å¯¾ç­– |
| :--- | :--- | :--- |
| **å…¨ä»¶å–å¾—** | `SELECT * FROM tokens` | **å±é™º**ã€‚æ•°åƒä¸‡ã€œæ•°å„„è¡Œã‚’ãƒ¡ãƒ¢ãƒªã«å±•é–‹ã—ã‚ˆã†ã¨ã—ã¦ Colab ãŒã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã—ã¾ã™ã€‚å¿…ãš `WHERE` ã‹ `LIMIT` ã‚’ã¤ã‘ã¦ãã ã•ã„ã€‚ |
| **è©³ç´°æƒ…å ±ã®æ¤œç´¢** | `WHERE token_info LIKE '%èª­ã¿%'` | JSON æ–‡å­—åˆ—ã®ä¸­èº«æ¤œç´¢ã¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒåŠ¹ãã¾ã›ã‚“ã€‚å…¨è¡Œã‚¹ã‚­ãƒ£ãƒ³ãŒç™ºç”Ÿã—ã€éžå¸¸ã«é…ããªã‚Šã¾ã™ã€‚ |
| **é–¢æ•°ã®ä½¿ç”¨** | `WHERE length(word) > 5` | ã‚«ãƒ©ãƒ ã«å¯¾ã—ã¦é–¢æ•°ï¼ˆ`length`ãªã©ï¼‰ã‚’ä½¿ã†ã¨ã€åŸºæœ¬çš„ã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒä½¿ã‚ã‚Œãšå…¨è¡Œè¨ˆç®—ã«ãªã‚Šã¾ã™ã€‚ |
| **ã‚ã„ã¾ã„æ¤œç´¢** | `WHERE word LIKE '%çŒ«%'` | `%` ã§å§‹ã¾ã‚‹ã‚ã„ã¾ã„æ¤œç´¢ï¼ˆä¸­é–“ä¸€è‡´ãƒ»å¾Œæ–¹ä¸€è‡´ï¼‰ã¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒåŠ¹ãã«ãã„ã§ã™ã€‚`word = 'çŒ«'`ï¼ˆå®Œå…¨ä¸€è‡´ï¼‰ã‚„ `word LIKE 'çŒ«%'`ï¼ˆå‰æ–¹ä¸€è‡´ï¼‰ã‚’æŽ¨å¥¨ã—ã¾ã™ã€‚ |

### ðŸ’¡ Python ã§å‡¦ç†ã™ã‚‹å ´åˆã®ã‚³ãƒ„

ã€Œç‰¹å®šã®å˜èªžã‚’å«ã¾ãªã„è¡Œã€ã‚„ã€Œè¤‡é›‘ãªæ¡ä»¶ã€ã§çµžã‚Šè¾¼ã¿ãŸã„å ´åˆã¯ã€SQL ã ã‘ã§é ‘å¼µã‚‰ãšã€**ã€Œdoc_id ã§å°‘ã—ãšã¤å–ã‚Šå‡ºã—ã¦ Python ã§å‡¦ç†ã™ã‚‹ã€** ã®ãŒæœ€ã‚‚å®‰å…¨ã§ç¢ºå®Ÿã§ã™ã€‚

```python
# è‰¯ã„ä¾‹ï¼š1000æ–‡æ›¸ãšã¤èª­ã¿è¾¼ã‚“ã§ã€Python (Pandas) ã§è¤‡é›‘ãªãƒ•ã‚£ãƒ«ã‚¿ã‚’ã™ã‚‹
chunk_size = 1000
for start_id in range(1, max_id + 1, chunk_size):
    # doc_id ã®ç¯„å›²æŒ‡å®šã¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒåŠ¹ãã®ã§é«˜é€Ÿ
    df_chunk = pd.read_sql(
        f"SELECT * FROM tokens WHERE doc_id >= {start_id} AND doc_id < {start_id + chunk_size}", 
        con
    )
    
    if df_chunk.empty:
        continue
        
    # Pythonã®ãƒ¡ãƒ¢ãƒªä¸Šã§è¤‡é›‘ãªçµžã‚Šè¾¼ã¿ï¼ˆã“ã“ã¯è‡ªç”±è‡ªåœ¨ï¼ï¼‰
    # ä¾‹ï¼štoken_info ã®ä¸­ã®æ–‡å­—åˆ—ã‚’æ¤œç´¢ã™ã‚‹ãªã©
    # target = df_chunk[df_chunk['token_info'].str.contains(...)]
